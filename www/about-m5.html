<html>
<head>
<title>About ICANN’s ITHI Metric M5</title>

<link rel="stylesheet" type="text/css" href="ithistyle.css">

<script src="ithinav.js"></script>
<script type="text/javascript">
    function init() {
        initnav();
    }
</script>
</head>

<body onload="init()">
    <nav id="navMenu"></nav>
    <h1>ITHI M5: DNS Recursive Resolver Integrity</h1>
    <p>
        The M5 metrics measure the "integrity" with which resolvers process queries.
        There are six components in the M5 metric:
    </p>
    <ul>
        <li>The metric M5.1 measures the % of visible recursive resolvers that refresh their local cache in accordance with the time specified in the SOA TTL, and the effect on users. For each population, we compute the fraction of users or resolvers that re-fetch erlier than the stated TTL, and the fraction that re-fetch according to the TTL. These two numbers do not add to 100%, because there is also a fraction of resolvers for which our measurements are inconclusive.</li>
        <li>The metric M5.2 measure the % of visible recursive resolvers that refresh their local cache automatically (without a triggering user query), as well as the estimated query load % due to automated cache refresh and the % of users using resolvers that auto-refresh their cache.</li>
        <li>The Metric M5.3 measures the % of visible recursive resolvers that set the DO (DNSSEC OK) bit in their queries, as well as the fraction of users using resolvers that set the DO bit in queries.</li>
        <li>The Metric M5.4 measures the % of visible recursive resolvers that perform DNSSEC validation of responses, as well as the fraction of users using resolvers that perform DNSSEC validation of responses.</li>
        <li>The metric M5.5 provide a distribution of the estimated client size of the 10,000 largest visible recursive resolvers.</li>
        <li>The metrics M5.6 and M5.7 estimates the concentration of the resolver services.</li>
    </ul>
    <h3>The APNIC polling process</h3>
    <p>
        The M5 metric is computed by APNIC. The process involves buying Google ads to generate a large number
        of "impressions" every day. The count varies from day to day, as explained <a href="https://stats.labs.apnic.net/about/">
            here
        </a>. Each ad runs a script that causes a script to be run, which resolves DNS names and loads web pages under APNIC
        control. The URL of the web page is personalized, enabling APNIC to correlate the traffic seen at the DNS resolver
        and web server with the script run at the client. The source address of the DNS queries received by APNIC
        identifies the resolver. The country in which the client is located is deduced for the source IP address
        of the web requests, using an IP address geolocation database.
    </p>
    <p>
        The statistics provide tabulation "per user" and "per resolver". The tabulation per resolver is straightforward,
        as each resolver can be identified by its IP address. The computation per user would be straightforward if
        the Google Ads were a randomized sampling of all Internet users, but that's not the case. Ad distribution is affected
        by a number of factor, resulting in oversampling of some countries, and undersampling of some others.
        APNIC compensate against by computing the number of impression per country, based on the IP addresses of
        the clients, and then computing a "per country" weight defined as the ratio of the number of Internet users
        in the country divided by the number of impressions in that country.
    </p>
    <h3>Individual resolvers and resolver services</h3>
    <p>
        The first 5 metrics identify and count resolvers by their individual IP addresses. This is not sufficient to
        assess the concentration of the resolver services, because a single service will often field several
        individual resolvers. We need to group the individual resolvers into clusters corresponding to separate services.
        We do that using three methods:
    </p>
    <ul>
        <li>
            Recognize all the IP addresses that have been associated with an open resolver
            as belonging to a single service. We have used the list at established by
            the <a hhref="https://www.publicdns.xyz">Public DNS project</a> to define
            the open DNS resolvers. This project established the list of open resolver
            IP addresses by measurements using <a href="https://atlas.ripe.net/">
                RIPE Atlas
            </a>.
        </li>

        <li>
            Check the remaining addresses and group them as single resolvers if they sit
            within a common subnet (a /24 in IPv4 and a /48 in IPv6). We used this in preference
            to testing whether they belong to the same BGP Autonomous System because looking
            at ASes tended to treat unrelated resolvers as a single service.
        </li>

        <li>
            Treat the remaining addresses as corresponding to individual services.
        </li>
    </ul>
    <p>
        This grouping is a bit arbitrary. There will be cases where different services, for example different ISP,
        run multiple resolvers at different locations, in different IPv4 or IPv6 subnets. They are counted as many
        resolvers using the definition above, but would be counted as a single service if we grouped addresses
        routed via BGP to the same Autonomous System (AS). The problem of course is that if grouping just
        by subnet is possibly too narrow, grouping by AS is too wide. For example, if an ISP serves several
        businesses that run their own resolvers, all these resolvers will be lumped in a single in the grouping
        per AS. Since we have not found the perfect grouping yet, we compute two series of numbers, one with the
        "subnet" grouping of resolvers, and one with the "AS" grouping.
    </p>
    <h3>Linking users to services</h3>
    <p>
        The web pages used by the script generate a different URL for each user, with a unique domain name and
        a known TTL. APNIC can observe DNS requests trying to "refresh the cache" for these domain names, and can infer
        from there the cache refresh strategies used by the resolvers.
    </p>
    <p>
        The metrics M5.6 and M5.7 measure the concentration of the resolver services by tracking the number
        of resolvers required to serve 50% or 90% of the number of clients.
        It turns out that the
        link between clients and resolvers is a bit fuzzy for two reasons:
    </p>
    <ul>
        <li>
            Some clients send their first queries to one resolver, but if the resolver does
            not respond they may try other resolvers. We thus distinguish "first use" resolver
            and "all" resolvers.
        </li>
        <li>
            Some clients send their queries in parallel to several resolvers. We thus distinguish
            between the set of resolvers that a fraction of client "sees" at least once, and the
            larger set resolvers that includes all the resolvers that a client "uses".
        </li>
    </ul>
    <p>
        This results in several variants of the M5.6 metrics:
    </p>
    <ul>
        <li>
            Metrics M5.6.1 and M5.6.2 measure the number of first use resolvers "seen" by 50% of
            users (M5.6.1) or 90% of users (M5.6.2)
        </li>
        <li>
            Metrics M5.6.3 and M5.6.4 measure the number of first use resolvers "used" by 50% of
            users (M5.6.3) or 90% of users (M5.6.4)
        </li>
        <li>
            Metrics M5.6.5 and M5.6.6 measure the number of all resolvers "seen" by 50% of
            users (M5.6.5) or 90% of users (M5.6.6)
        </li>
        <li>
            Metrics M5.6.7 and M5.6.8 measure the number of all resolvers "used" by 50% of
            users (M5.6.7) or 90% of users (M5.6.8)
        </li>
        <li>
            Metrics M5.6.9 to M5.6.16 measure the same numbers as M5.6.1 to M5.6.8, but
            apply a grouping "per AS" instead of the grouping "per subnet" used in the
            previous metrics.
        </li>
    </ul>
    <p>
        We illustrate these computations with an example network of just three clients and four resolvers, each belonging to different services:
    </p>
    <img src="explain-m56.png"
         alt="Graph showing hypothetical clients and resolvers and the resulting groupings"
         style="float:right;width:25%;margin:0px 10px" />
    <ul>
        <li>
            The client X sends its first queries in parallel to resolvers A and B,
        </li>
        <li>
            The client Y sends its first queries to resolver C and may use resolver B as a backup,
        </li>
        <li>
            The client Z sends its first queries in parallel to B, and may use C and D as a backup.
        </li>
    </ul>
    <p>
        The purpose of the example is to explain the differences between what we call "first" and "all",
        or "used" and "seen". We compute the smallest set of resolvers that are seen the first queries of every client,
        that are used in the first queries of every client, that are seen in all queries of every client,
        and that are used in all queries of every client. We see that these are four different sets:
    </p>
    <ul>
        <li>The smallest set is the minimal set of resolvers that are seen in all queries of every client. In our example, the set only includes resolver B.</li>
        <li>The next smallest set is the minimal set of resolvers that are seen in all first queries of every client. This set includes resolvers B and C.</li>
        <li>The next set is the minimal set of resolvers that are used in all first queries of every client. This set includes resolvers A, B and C.</li>
        <li>The largest set is the minimal set of resolvers that are used in all queries of every client. This set includes resolvers A, B, C, and C.</li>
    </ul>
    <p>
        The computation of the metrics M5.6 follows the same logic, but instead of measuring the set that serves "every" client, we look at the minimal sets that are seen or used by 50% or 90% of clients.
    </p>
    <p>
        The metric M5.7 tries to simplify the approach by considering only the first
        query to arrive at the test server after a client resolution request. If that request
        triggered several parallel queries, only the first to arrive ot "initial" query
        is considered, and the other are ignored.
    </p>
    <ul>
        <li>
            Metrics M5.7.1 and M5.7.2 measure the number of resolvers accounting by 50% of
            users (M5.7.1) or 90% of users (M5.7.2), considering only "initial" queries.
        </li>
        <li>
            Metrics M5.7.9 and M5.7.10 measure the same numbers as M5.7.1 and M5.7.2, but
            apply a grouping "per AS" instead of the grouping "per subnet" used in the
            previous metrics.
        </li>
    </ul>
    <h3>Complete list of M5 metrics</h3>
    <p>
        The complete list of M5 metrics is listed in the following table:
    </p>

    <table class="metrics">
        <tr>
            <th colspan=2>M5.1 (% alignment of cache time to SOA TTL)</th>
        </tr>
        <tr>

            <td class="number">
                M5.1.1
            </td>
            <td>
                % of users using resolvers that re-fetch early
            </td>
        </tr>
        <tr>
            <td class="number">
                M5.1.2
            </td>
            <td>
                % of users using resolvers that re-fetch according to TTL
            </td>
        </tr>
        <tr>
            <td class="number">
                M5.1.3
            </td>
            <td>
                % of users using indeterminate resolvers
            </td>
        </tr>
        <tr>
            <td class="number">
                M5.1.4
            </td>
            <td>
                % of resolvers that re-fetch early
            </td>
        </tr>
        <tr>
            <td class="number">
                M5.1.5
            </td>
            <td>
                % of resolvers that re-fetch according to TTL
            </td>
        </tr>
        <tr>
            <td class="number">
                M5.1.6
            </td>
            <td>
                % of resolvers where cache time is indeterminate
            </td>
        </tr>
        <tr></tr>
        <tr>
            <th colspan=2>
                M5.2 (% auto cache refresh)
            </th>
        </tr>
        <tr>
            <td class="number">
                M5.2.1
            </td>
            <td>
                % of users using resolvers that auto-refresh their cache
            </td>
        </tr>

        <tr>
            <td class="number">
                M5.2.2
            </td>
            <td>
                % of query load due to cache refresh
            </td>
        </tr>

        <tr>
            <td class="number">
                M5.2.3
            </td>
            <td>
                % of resolvers that auto-refresh their cache
            </td>
        </tr>
        <tr>
            <th colspan=2>
                M5.3 (% EDNS0 DO query rate)
            </th>
        </tr>

        <tr>
            <td class="number">
                M5.3.1
            </td>
            <td>
                % of users using resolvers that set the DO bit in queries
            </td>
        </tr>

        <tr>
            <td class="number">
                M5.3.2
            </td>
            <td>
                % of resolvers that set the DO bit in queries
            </td>
        </tr>
        <tr></tr>
        <tr>
            <th colspan=2>
                M5.4 (%DNSSEC Validation rate)
            </th>
        </tr>

        <tr>
            <td class="number">
                M5.4.1
            </td>
            <td>
                % of users using resolvers that perform DNSSEC validation
            </td>
        </tr>

        <tr>
            <td class="number">
                M5.4.2
            </td>
            <td>
                % of resolvers that perform DNSSEC validation
            </td>
        </tr>
        <tr></tr>
        <tr>
            <th colspan=2>
                M5.5 (Distribution of resolver use)
            </th>
        </tr>

        <tr>
            <td class="number">
                M5.5
            </td>
            <td>
                % of users using one of the top 10,000 resolvers
            </td>
        </tr>
        <tr></tr>
        <tr>
            <th colspan=2>
                M5.6 (Concentration of the resolver service)
            </th>
        </tr>

        <tr>
            <td class="number">
                M5.6.1
            </td>
            <td>
                Number of first use resolvers seen by 50% of users
            </td>
        </tr>

        <tr>
            <td class="number">
                M5.6.2
            </td>
            <td>
                Number of first use resolvers seen by 90% of users
            </td>
        </tr>

        <tr>
            <td class="number">
                M5.6.3
            </td>
            <td>
                Number of first use resolvers used by 50% of users
            </td>
        </tr>

        <tr>
            <td class="number">
                M5.6.4
            </td>
            <td>
                Number of first use resolvers used by 90% of users
            </td>
        </tr>

        <tr>
            <td class="number">
                M5.6.5
            </td>
            <td>
                Number of resolvers seen by 50% of users
            </td>
        </tr>

        <tr>
            <td class="number">
                M5.6.5
            </td>
            <td>
                Number of resolvers seen by 90% of users
            </td>
        </tr>

        <tr>
            <td class="number">
                M5.6.7
            </td>
            <td>
                Number of resolvers used by 50% of users
            </td>
        </tr>

        <tr>
            <td class="number">
                M5.6.8
            </td>
            <td>
                Number of resolvers used by 90% of users
            </td>
        </tr>
        <tr>
            <td class="number">
                M5.7.1
            </td>
            <td>
                Number of initial resolvers seen by 50% of users
            </td>
        </tr>

        <tr>
            <td class="number">
                M5.7.2
            </td>
            <td>
                Number of initial resolvers seen by 90% of users
            </td>
        </tr>

    </table>

    <p>
        The current values of the metrics is available <a href="graph-m5.html">here</a>.
    </p>
</body>
</html>